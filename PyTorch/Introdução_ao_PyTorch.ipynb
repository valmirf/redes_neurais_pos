{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introdução ao PyTorch",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/redes_neurais_pos/blob/main/PyTorch/Introdu%C3%A7%C3%A3o_ao_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bmMqzUbgrL2"
      },
      "source": [
        "##Frameworks\n",
        "\n",
        "---\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais_ple/master/PyTorch/FIG/frameworks.png)\n",
        "\n",
        "\n",
        "#Empresas envolvidas\n",
        "\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais_ple/master/PyTorch/FIG/frameworks2.png)\n",
        "\n",
        "\n",
        "#PyTorch\n",
        "\n",
        "PyTorch é um framework de Python que funciona como uma camada em cima do Tensorflow, facilitando seu uso e facilitando procedimentos no uso de redes neurais artificiais e GPU. PyTorch tem se tornado padrão em várias empresas e institutos de pesquisa.\n",
        "\n",
        "Nesse tutorial, vamos aprender a usar o PyTorch no treinamento e utilização de uma rede neural artificial. O tutorial seguirá várias etapas, desde a leitura de uma base de dados, até o treinamento e avaliação da rede neural treinada. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtGDIv9NDnBW"
      },
      "source": [
        "#Import packages\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import time\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "import time\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "#Test if GPU Cuda is available\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(cuda_available)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQs7GKbgwc9w"
      },
      "source": [
        "###Nesse exercício, nós treinaremos uma rede MLP para classificar imagens da base MNIST.\n",
        "\n",
        "\n",
        "\n",
        "# 1. Carregar uma base de dados no PyTorch\n",
        "\n",
        "A biblioteca torchvision.datasets possui as principais bases de dados disponíveis para download no PyTorch (ver: https://pytorch.org/docs/stable/torchvision/datasets.html)\n",
        "\n",
        "Algumas bases de dados:\n",
        "\n",
        "*   MNIST\n",
        "*   COCO\n",
        "*   ImageNet\n",
        "*   CIFAR\n",
        "*   Flickr\n",
        "*   VOC\n",
        "*   Cityscapes\n",
        "\n",
        "\n",
        "\n",
        "Iremos usar a função DataLoader importada de TorchVision para carregar a base de dados MNIST. Usaremos um lote (batch) de tamanho 64 para treinamento e tamanho 1000 para teste neste conjunto de dados. O módulo TorchVision oferece ainda, muitas transformações úteis, como corte ou normalização. Os valores 0.1307 e 0.3081 são usados para a função Normalize(). Esses valores correspondem a média global e o desvio padrão do conjunto de dados MNIST. \n",
        "\n",
        "Usando o DataLoader, também poderíamos usar num_workers> 1, para usar subprocessos para carregar dados de forma assíncrona ou usar uma parte da memória RAM fixa (via pin_memory), para acelerar as transferências de RAM para GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtuotbE9wVZ4"
      },
      "source": [
        "batch_size=64\n",
        "cuda=0\n",
        "\n",
        "\n",
        "#normalize with mean and standard deviation\n",
        "normalize = transforms.Normalize(mean=(0.1307,),std=(0.3081,))\n",
        "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "# define three datasets in order to have different transforms\n",
        "# on training, validation and test\n",
        "dataset_train = datasets.MNIST('/data/mnist/train', download=True, train=True, transform=transform)\n",
        "#dataset_val = datasets.MNIST('/data/mnist/val', download=True, train=True, transform=transform)\n",
        "dataset_test = datasets.MNIST('/data/mnist/test', download=True, train=False, transform=transform)\n",
        "\n",
        " \n",
        "trainloader = torch.utils.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
        "#valloader = torch.utils.data.DataLoader(dataset_val, batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(dataset_test, test_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYpHhBH7Jq3p"
      },
      "source": [
        "Agora, vamos dar uma olhada em alguns exemplos da base de dados de teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foKuV0DxJ5YS"
      },
      "source": [
        "examples = enumerate(testloader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ubYVYDJ_72"
      },
      "source": [
        "Vamos ver em que consiste um lote de dados de teste. Portanto, um lote de dados do conjunto de teste é um tensor de forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMLzUiuVKBPt"
      },
      "source": [
        "example_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsLIR-o5KJ8Q"
      },
      "source": [
        "Isso significa que temos 1000 exemplos de 28x28 pixels em escala de cinza (ou seja, sem canais RGB, portanto, um). Podemos plotar alguns deles usando o matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heIdoZ2iKONW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Rótulo: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRlCLDdCw6jE"
      },
      "source": [
        "Ver a imagem em mais detalhes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L657_et1w1Bt"
      },
      "source": [
        "def visualize_input(img, ax): \n",
        "    #img = np.uint8(img)\n",
        "    #print('img: ', img)\n",
        "    max = img.max()\n",
        "    min = img.min()\n",
        "    data = (img-min) / (max-min) # normalize the data to 0 - 1\n",
        "    data = 255 * data # Now scale by 255\n",
        "    img = np.uint8(data)\n",
        "    #img = np.uint8(img.mul(255).numpy())\n",
        "    #print('img: ', np.uint8(img.numpy()))\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    width, height = img.shape\n",
        "    thresh = img.max()/2.5\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            #ax.annotate(str(round(img[x][y],2)), xy=(y,x),\n",
        "            ax.annotate(str(img[x][y]), xy=(y,x),\n",
        "                        horizontalalignment='center',\n",
        "                        verticalalignment='center',\n",
        "                        color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "fig = plt.figure(figsize = (12,12)) \n",
        "ax = fig.add_subplot(111)\n",
        "visualize_input(example_data[0][0], ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0-sLa-0yN3p"
      },
      "source": [
        "# 2. Definir o Modelo de Arquitetura\n",
        "\n",
        "Nessa etapa, vamos definir o modelo da arquitetura da rede neural. O modelo criado terá uma camada de entrada, uma camada de saída de dez neurônios e duas camadas ocultas entre elas, uma com 128 e outra com 64 neurônios.\n",
        "\n",
        "O módulo torch.nn permite constuir a arquitetura da rede de forma bastante simples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G-jA1tCyM7S"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## Define the NN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        # linear layer (n_hidden -> hidden_2)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        # linear layer (n_hidden -> 10)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        # dropout layer (p=0.2)\n",
        "        # dropout prevents overfitting of data\n",
        "        #self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = Net()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTqksJ8O5bT2"
      },
      "source": [
        "O módulo nn.Sequential envolve as camadas na rede. Existem duas camadas lineares com a ativação da ReLU (uma função simples que permite a passagem de valores positivos, enquanto valores negativos são modificados para zero). A camada de saída é uma camada linear com a ativação da função LogSoftmax. Tecnicamente, uma função LogSoftmax é o logaritmo de uma função Softmax.\n",
        "\n",
        "Além disso, temos 784 unidades na primeira camada, porque redimensionamos cada imagem antes de enviá-la para dentro da rede neural. (28 x 28 = 784)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7CIhbnzMJEK"
      },
      "source": [
        "class Net2(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(Net2, self).__init__()\n",
        "    self.seq = nn.Sequential(\n",
        "               nn.Linear(input_dim,hidden_dim),\n",
        "               nn.ReLU(),\n",
        "               nn.Linear(hidden_dim,output_dim),\n",
        "               nn.LogSoftmax(dim=1)\n",
        "               )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # flatten image input\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    return self.seq(x)\n",
        "\n",
        "# initialize the NN\n",
        "#model = Net2(28*28,256,10)\n",
        "#model = Net2()\n",
        "#print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BhYorHa1paI"
      },
      "source": [
        "# 3. Modelo de otimização\n",
        "\n",
        "Em seguida, definimos a função de perda como a probabilidade de log negativa. Juntas, as funções LogSoftmax() e o NLLLoss(), atuam como a perda de entropia cruzada com SoftMax.\n",
        "\n",
        "\n",
        "## 3.1 Define o otimizador (SGD)\n",
        "\n",
        "[https://pytorch.org/optimizer](https://pytorch.org/cppdocs/api/classtorch_1_1optim_1_1_optimizer.html)\n",
        "\n",
        "### Tipos de otimizadores:\n",
        "*   SGD\n",
        "*   RMSprop\n",
        "*   Adagrad\n",
        "*   Adam\n",
        "\n",
        "\n",
        "## 3.2 Especifica função de perda (categorical_crossentropy)\n",
        "\n",
        "[https://pytorch.org/nn/](https://pytorch.org/docs/stable/nn.html#)\n",
        "\n",
        "### Tipos de funções de perda\n",
        "*   mean_squared_error \n",
        "*   mean_absolute_error\n",
        "*   mean_absolute_percentage_error\n",
        "*   mean_squared_logarithmic_error\n",
        "*   squared_hinge\n",
        "*   hinge\n",
        "*   categorical_hinge\n",
        "*   logcosh\n",
        "*   categorical_crossentropy\n",
        "*   sparse_categorical_crossentropy\n",
        "*   binary_crossentropy\n",
        "*   kullback_leibler_divergence\n",
        "*   poisson\n",
        "*   cosine_proximity\n",
        "\n",
        "\n",
        "## 3.3 Como escolher uma função de perda?\n",
        "\n",
        "### Funções de perda para regressão\n",
        "*   Mean Squared Error\n",
        "*   Mean Squared Logarithmic Error\n",
        "*   Mean Absolute Error\n",
        "\n",
        "### Funções de perda para classificação binária\n",
        "*   Binary Cross-Entropy\n",
        "*   Hinge\n",
        "*   Squared Hinge \n",
        "\n",
        "### Funções de perda para classificação multiclasse\n",
        "*   Multi-Class Cross-Entropy\n",
        "*   Sparse Multiclass Cross-Entropy\n",
        "*   Categorical Hinge\n",
        "*   Kullback Leibler Divergence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8XnZ_lp1QYe"
      },
      "source": [
        "\n",
        "## Specify loss and optimization functions\n",
        "\n",
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFzLrHLmzqsE"
      },
      "source": [
        "# 4. Treinar o Modelo\n",
        "\n",
        "Nessa etapa, o treinamento da rede neural é realizado. Sua rede neural repete o conjunto de treinamento e atualiza os pesos. Utilizamos o módulo torch.optim, que é um módulo do PyTorch para otimizar o modelo. Nessa arquitetura, usamos a descida de gradiente e os pesos são atualizados por backpropagation. Assim, em cada época (número de vezes que repetimos o conjunto de treinamento), veremos uma diminuição gradual do valor da função de perda.\n",
        "\n",
        "Observe que para a fase de treinamento, o modelo está no modo de treinamento - (model.train()) e também precisamos zerar os gradientes de cada lote. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msa7ktaOzaTp"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 10  # suggest training between 20-50 epochs\n",
        "\n",
        "model.train() # prep model for training\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for data, target in trainloader:\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        #data = images.view(data.shape[0], -1)\n",
        "\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "    else:\n",
        "        # print training statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = train_loss/len(trainloader)\n",
        "        print(\"Epoch {} - Training loss: {}\".format(epoch, train_loss/len(trainloader)))\n",
        "        #print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)        \n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Aj0wRy30NFF"
      },
      "source": [
        "# 5. Avaliando o modelo no conjunto de teste\n",
        "\n",
        "Agora, o modelo treinado na etapa anterior será avaliado. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHPrlVga4R_B"
      },
      "source": [
        "images, labels = next(iter(testloader))\n",
        "\n",
        "img = images[0].view(1, 784)\n",
        "\n",
        "with torch.no_grad():\n",
        "  logps = model(img)\n",
        "\n",
        "ps = torch.exp(logps)\n",
        "probab = list(ps.numpy()[0])\n",
        "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
        "print(\"Digit =\", labels.numpy()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEwjOippACA9"
      },
      "source": [
        "Ness etapa, iteramos o conjunto de teste e calculamos o número total de previsões corretas. \n",
        "\n",
        "Observe que precisamos especificar que estamos usando o modelo apenas para avaliação - (model.eval()). Não é preciso calcular os gradientes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01W_zbN24xqX"
      },
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval() # prep model for *evaluation*\n",
        "\n",
        "for data, target in testloader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(testloader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05npcHm3AV7-"
      },
      "source": [
        " ## 6. Salvando o modelo\n",
        "\n",
        "Para salvar o modelo treinado, para que não seja preciso treinar novamente quando for usar o modelo, salvamos o modelo. Quando precisarmos no futuro, podemos carregá-lo e usá-lo diretamente, sem treinamento adicional.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhRlEyBNAp38"
      },
      "source": [
        "torch.save(model, './my_mnist_model.pt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCCDsib8YKkL"
      },
      "source": [
        "#TESTES\n",
        "1. Modifique a quantidade de camadas do modelo treinado\n",
        "2. Modifique a quantidade de neurônios na(s) camada(s) intermediárias do modelo treinado\n",
        "3. Modifique o tipo de otimizador do seu modelo\n",
        "4. Modifique a função de perda do seu modelo\n",
        "5. Avalie combinações entre tipos de otimizador, função de perda, tipos de camadas e quantidade de épocas no treinamento. Qual foi a melhor taxa de acerto no conjunto de teste? Qual a melhor configuração de modelo que você encontrou? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyvg8mBc46LF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}